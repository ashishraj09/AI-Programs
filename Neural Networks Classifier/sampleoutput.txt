import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report
â€‹
wineTrain = pd.read_csv('wine_training', sep = ' ')
wineTest = pd.read_csv('wine_test', sep = ' ')
x_Train=wineTrain.drop('Class', axis = 1)
y_Train= wineTrain['Class']
x_Test=wineTest.drop('Class', axis = 1)
y_Test=wineTest['Class']
scaler = StandardScaler()
scaler.fit(x_Train)
StandardScaler(copy=True, with_mean=True, with_std=True)
x_Train = scaler.transform(x_Train)
x_Test = scaler.transform(x_Test)
clf = MLPClassifier(hidden_layer_sizes=(11), max_iter=1000,random_state = 40, verbose= True)
clf.fit(x_Train, y_Train)
Iteration 1, loss = 1.53069471
Iteration 2, loss = 1.51636464
Iteration 3, loss = 1.50217945
Iteration 4, loss = 1.48814417
Iteration 5, loss = 1.47422052
Iteration 6, loss = 1.46043576
Iteration 7, loss = 1.44673390
Iteration 8, loss = 1.43311068
Iteration 9, loss = 1.41958273
Iteration 10, loss = 1.40617891
Iteration 11, loss = 1.39290463
Iteration 12, loss = 1.37975642
Iteration 13, loss = 1.36677399
Iteration 14, loss = 1.35390624
Iteration 15, loss = 1.34108889
Iteration 16, loss = 1.32837598
Iteration 17, loss = 1.31578909
Iteration 18, loss = 1.30335066
Iteration 19, loss = 1.29103988
Iteration 20, loss = 1.27887128
Iteration 21, loss = 1.26678428
Iteration 22, loss = 1.25481789
Iteration 23, loss = 1.24297477
Iteration 24, loss = 1.23131634
Iteration 25, loss = 1.21983484
Iteration 26, loss = 1.20847874
Iteration 27, loss = 1.19726529
Iteration 28, loss = 1.18616677
Iteration 29, loss = 1.17520836
Iteration 30, loss = 1.16435417
Iteration 31, loss = 1.15366073
Iteration 32, loss = 1.14312360
Iteration 33, loss = 1.13267163
Iteration 34, loss = 1.12233428
Iteration 35, loss = 1.11222054
Iteration 36, loss = 1.10224452
Iteration 37, loss = 1.09237964
Iteration 38, loss = 1.08262455
Iteration 39, loss = 1.07299961
Iteration 40, loss = 1.06349334
Iteration 41, loss = 1.05413633
Iteration 42, loss = 1.04489171
Iteration 43, loss = 1.03574212
Iteration 44, loss = 1.02665189
Iteration 45, loss = 1.01766076
Iteration 46, loss = 1.00876546
Iteration 47, loss = 0.99997091
Iteration 48, loss = 0.99127464
Iteration 49, loss = 0.98266598
Iteration 50, loss = 0.97415338
Iteration 51, loss = 0.96578057
Iteration 52, loss = 0.95753900
Iteration 53, loss = 0.94939356
Iteration 54, loss = 0.94127135
Iteration 55, loss = 0.93327746
Iteration 56, loss = 0.92540139
Iteration 57, loss = 0.91764178
Iteration 58, loss = 0.90997798
Iteration 59, loss = 0.90244889
Iteration 60, loss = 0.89502048
Iteration 61, loss = 0.88766737
Iteration 62, loss = 0.88037361
Iteration 63, loss = 0.87316510
Iteration 64, loss = 0.86598543
Iteration 65, loss = 0.85889564
Iteration 66, loss = 0.85188375
Iteration 67, loss = 0.84494307
Iteration 68, loss = 0.83802270
Iteration 69, loss = 0.83113476
Iteration 70, loss = 0.82431500
Iteration 71, loss = 0.81757045
Iteration 72, loss = 0.81091232
Iteration 73, loss = 0.80432732
Iteration 74, loss = 0.79781330
Iteration 75, loss = 0.79136809
Iteration 76, loss = 0.78493146
Iteration 77, loss = 0.77849120
Iteration 78, loss = 0.77204763
Iteration 79, loss = 0.76565962
Iteration 80, loss = 0.75936434
Iteration 81, loss = 0.75315159
Iteration 82, loss = 0.74697524
Iteration 83, loss = 0.74079123
Iteration 84, loss = 0.73464538
Iteration 85, loss = 0.72856759
Iteration 86, loss = 0.72255897
Iteration 87, loss = 0.71664124
Iteration 88, loss = 0.71081397
Iteration 89, loss = 0.70505612
Iteration 90, loss = 0.69936951
Iteration 91, loss = 0.69375420
Iteration 92, loss = 0.68821477
Iteration 93, loss = 0.68274573
Iteration 94, loss = 0.67733027
Iteration 95, loss = 0.67198510
Iteration 96, loss = 0.66665501
Iteration 97, loss = 0.66136358
Iteration 98, loss = 0.65612779
Iteration 99, loss = 0.65095619
Iteration 100, loss = 0.64583990
Iteration 101, loss = 0.64076571
Iteration 102, loss = 0.63574421
Iteration 103, loss = 0.63077732
Iteration 104, loss = 0.62586869
Iteration 105, loss = 0.62101798
Iteration 106, loss = 0.61618683
Iteration 107, loss = 0.61140538
Iteration 108, loss = 0.60665919
Iteration 109, loss = 0.60193806
Iteration 110, loss = 0.59725220
Iteration 111, loss = 0.59257154
Iteration 112, loss = 0.58793492
Iteration 113, loss = 0.58333912
Iteration 114, loss = 0.57877230
Iteration 115, loss = 0.57424778
Iteration 116, loss = 0.56971888
Iteration 117, loss = 0.56521004
Iteration 118, loss = 0.56071822
Iteration 119, loss = 0.55625436
Iteration 120, loss = 0.55178260
Iteration 121, loss = 0.54726121
Iteration 122, loss = 0.54275649
Iteration 123, loss = 0.53827212
Iteration 124, loss = 0.53375796
Iteration 125, loss = 0.52927076
Iteration 126, loss = 0.52482416
Iteration 127, loss = 0.52041684
Iteration 128, loss = 0.51605883
Iteration 129, loss = 0.51173853
Iteration 130, loss = 0.50745248
Iteration 131, loss = 0.50320661
Iteration 132, loss = 0.49900267
Iteration 133, loss = 0.49484132
Iteration 134, loss = 0.49072778
Iteration 135, loss = 0.48666994
Iteration 136, loss = 0.48266043
Iteration 137, loss = 0.47864723
Iteration 138, loss = 0.47464116
Iteration 139, loss = 0.47067358
Iteration 140, loss = 0.46666339
Iteration 141, loss = 0.46262976
Iteration 142, loss = 0.45863351
Iteration 143, loss = 0.45464995
Iteration 144, loss = 0.45069340
Iteration 145, loss = 0.44676948
Iteration 146, loss = 0.44287974
Iteration 147, loss = 0.43902459
Iteration 148, loss = 0.43520149
Iteration 149, loss = 0.43141521
Iteration 150, loss = 0.42766556
Iteration 151, loss = 0.42395399
Iteration 152, loss = 0.42028100
Iteration 153, loss = 0.41664736
Iteration 154, loss = 0.41304220
Iteration 155, loss = 0.40945764
Iteration 156, loss = 0.40591041
Iteration 157, loss = 0.40240012
Iteration 158, loss = 0.39892839
Iteration 159, loss = 0.39549399
Iteration 160, loss = 0.39209637
Iteration 161, loss = 0.38871567
Iteration 162, loss = 0.38534686
Iteration 163, loss = 0.38202148
Iteration 164, loss = 0.37873606
Iteration 165, loss = 0.37548907
Iteration 166, loss = 0.37227741
Iteration 167, loss = 0.36910336
Iteration 168, loss = 0.36596058
Iteration 169, loss = 0.36285092
Iteration 170, loss = 0.35977580
Iteration 171, loss = 0.35669766
Iteration 172, loss = 0.35362218
Iteration 173, loss = 0.35057612
Iteration 174, loss = 0.34756252
Iteration 175, loss = 0.34457458
Iteration 176, loss = 0.34158372
Iteration 177, loss = 0.33861080
Iteration 178, loss = 0.33566415
Iteration 179, loss = 0.33274441
Iteration 180, loss = 0.32985232
Iteration 181, loss = 0.32697835
Iteration 182, loss = 0.32411400
Iteration 183, loss = 0.32127614
Iteration 184, loss = 0.31846531
Iteration 185, loss = 0.31567833
Iteration 186, loss = 0.31288574
Iteration 187, loss = 0.31011797
Iteration 188, loss = 0.30735649
Iteration 189, loss = 0.30459342
Iteration 190, loss = 0.30185224
Iteration 191, loss = 0.29913386
Iteration 192, loss = 0.29643906
Iteration 193, loss = 0.29376847
Iteration 194, loss = 0.29112262
Iteration 195, loss = 0.28847709
Iteration 196, loss = 0.28583629
Iteration 197, loss = 0.28321559
Iteration 198, loss = 0.28061801
Iteration 199, loss = 0.27804422
Iteration 200, loss = 0.27549669
Iteration 201, loss = 0.27297778
Iteration 202, loss = 0.27047853
Iteration 203, loss = 0.26799938
Iteration 204, loss = 0.26554537
Iteration 205, loss = 0.26311787
Iteration 206, loss = 0.26071682
Iteration 207, loss = 0.25834417
Iteration 208, loss = 0.25599906
Iteration 209, loss = 0.25367950
Iteration 210, loss = 0.25138536
Iteration 211, loss = 0.24911647
Iteration 212, loss = 0.24687268
Iteration 213, loss = 0.24465393
Iteration 214, loss = 0.24246122
Iteration 215, loss = 0.24029301
Iteration 216, loss = 0.23814905
Iteration 217, loss = 0.23602904
Iteration 218, loss = 0.23393474
Iteration 219, loss = 0.23186490
Iteration 220, loss = 0.22981817
Iteration 221, loss = 0.22779425
Iteration 222, loss = 0.22579282
Iteration 223, loss = 0.22381242
Iteration 224, loss = 0.22184532
Iteration 225, loss = 0.21986331
Iteration 226, loss = 0.21789970
Iteration 227, loss = 0.21595478
Iteration 228, loss = 0.21403037
Iteration 229, loss = 0.21212686
Iteration 230, loss = 0.21024035
Iteration 231, loss = 0.20837268
Iteration 232, loss = 0.20652391
Iteration 233, loss = 0.20468364
Iteration 234, loss = 0.20284871
Iteration 235, loss = 0.20103137
Iteration 236, loss = 0.19923210
Iteration 237, loss = 0.19745117
Iteration 238, loss = 0.19568897
Iteration 239, loss = 0.19392159
Iteration 240, loss = 0.19215680
Iteration 241, loss = 0.19040798
Iteration 242, loss = 0.18867560
Iteration 243, loss = 0.18696008
Iteration 244, loss = 0.18526192
Iteration 245, loss = 0.18358142
Iteration 246, loss = 0.18191443
Iteration 247, loss = 0.18023760
Iteration 248, loss = 0.17857659
Iteration 249, loss = 0.17693102
Iteration 250, loss = 0.17529978
Iteration 251, loss = 0.17368543
Iteration 252, loss = 0.17208825
Iteration 253, loss = 0.17050842
Iteration 254, loss = 0.16891839
Iteration 255, loss = 0.16733965
Iteration 256, loss = 0.16577633
Iteration 257, loss = 0.16422902
Iteration 258, loss = 0.16269817
Iteration 259, loss = 0.16118417
Iteration 260, loss = 0.15966784
Iteration 261, loss = 0.15813513
Iteration 262, loss = 0.15661592
Iteration 263, loss = 0.15511106
Iteration 264, loss = 0.15362128
Iteration 265, loss = 0.15214719
Iteration 266, loss = 0.15069020
Iteration 267, loss = 0.14925133
Iteration 268, loss = 0.14782947
Iteration 269, loss = 0.14642483
Iteration 270, loss = 0.14503759
Iteration 271, loss = 0.14366784
Iteration 272, loss = 0.14231562
Iteration 273, loss = 0.14097879
Iteration 274, loss = 0.13961159
Iteration 275, loss = 0.13823070
Iteration 276, loss = 0.13686217
Iteration 277, loss = 0.13550703
Iteration 278, loss = 0.13416616
Iteration 279, loss = 0.13284030
Iteration 280, loss = 0.13153003
Iteration 281, loss = 0.13023587
Iteration 282, loss = 0.12895845
Iteration 283, loss = 0.12769784
Iteration 284, loss = 0.12645422
Iteration 285, loss = 0.12522776
Iteration 286, loss = 0.12401304
Iteration 287, loss = 0.12281321
Iteration 288, loss = 0.12163042
Iteration 289, loss = 0.12045344
Iteration 290, loss = 0.11928879
Iteration 291, loss = 0.11813994
Iteration 292, loss = 0.11700712
Iteration 293, loss = 0.11589050
Iteration 294, loss = 0.11479016
Iteration 295, loss = 0.11370613
Iteration 296, loss = 0.11262964
Iteration 297, loss = 0.11156571
Iteration 298, loss = 0.11051733
Iteration 299, loss = 0.10948439
Iteration 300, loss = 0.10846704
Iteration 301, loss = 0.10746513
Iteration 302, loss = 0.10647859
Iteration 303, loss = 0.10550736
Iteration 304, loss = 0.10455129
Iteration 305, loss = 0.10361027
Iteration 306, loss = 0.10268439
Iteration 307, loss = 0.10177312
Iteration 308, loss = 0.10087623
Iteration 309, loss = 0.09999348
Iteration 310, loss = 0.09912440
Iteration 311, loss = 0.09825883
Iteration 312, loss = 0.09740602
Iteration 313, loss = 0.09656587
Iteration 314, loss = 0.09573824
Iteration 315, loss = 0.09492298
Iteration 316, loss = 0.09411995
Iteration 317, loss = 0.09332958
Iteration 318, loss = 0.09255117
Iteration 319, loss = 0.09178444
Iteration 320, loss = 0.09102917
Iteration 321, loss = 0.09028507
Iteration 322, loss = 0.08955202
Iteration 323, loss = 0.08882982
Iteration 324, loss = 0.08811825
Iteration 325, loss = 0.08741713
Iteration 326, loss = 0.08672631
Iteration 327, loss = 0.08604550
Iteration 328, loss = 0.08537450
Iteration 329, loss = 0.08471313
Iteration 330, loss = 0.08406246
Iteration 331, loss = 0.08342159
Iteration 332, loss = 0.08279025
Iteration 333, loss = 0.08216782
Iteration 334, loss = 0.08155409
Iteration 335, loss = 0.08094885
Iteration 336, loss = 0.08035193
Iteration 337, loss = 0.07976313
Iteration 338, loss = 0.07918229
Iteration 339, loss = 0.07860922
Iteration 340, loss = 0.07804376
Iteration 341, loss = 0.07748643
Iteration 342, loss = 0.07693664
Iteration 343, loss = 0.07639402
Iteration 344, loss = 0.07585840
Iteration 345, loss = 0.07533004
Iteration 346, loss = 0.07480844
Iteration 347, loss = 0.07429334
Iteration 348, loss = 0.07378465
Iteration 349, loss = 0.07328222
Iteration 350, loss = 0.07278593
Iteration 351, loss = 0.07229562
Iteration 352, loss = 0.07181120
Iteration 353, loss = 0.07133256
Iteration 354, loss = 0.07085957
Iteration 355, loss = 0.07039212
Iteration 356, loss = 0.06992799
Iteration 357, loss = 0.06946823
Iteration 358, loss = 0.06901346
Iteration 359, loss = 0.06856363
Iteration 360, loss = 0.06811868
Iteration 361, loss = 0.06767854
Iteration 362, loss = 0.06724316
Iteration 363, loss = 0.06681247
Iteration 364, loss = 0.06638641
Iteration 365, loss = 0.06596486
Iteration 366, loss = 0.06554781
Iteration 367, loss = 0.06513519
Iteration 368, loss = 0.06472693
Iteration 369, loss = 0.06432296
Iteration 370, loss = 0.06392323
Iteration 371, loss = 0.06352767
Iteration 372, loss = 0.06313622
Iteration 373, loss = 0.06274881
Iteration 374, loss = 0.06236538
Iteration 375, loss = 0.06198588
Iteration 376, loss = 0.06161086
Iteration 377, loss = 0.06123969
Iteration 378, loss = 0.06087227
Iteration 379, loss = 0.06050856
Iteration 380, loss = 0.06014827
Iteration 381, loss = 0.05979094
Iteration 382, loss = 0.05943707
Iteration 383, loss = 0.05908664
Iteration 384, loss = 0.05873958
Iteration 385, loss = 0.05839586
Iteration 386, loss = 0.05805542
Iteration 387, loss = 0.05772017
Iteration 388, loss = 0.05738986
Iteration 389, loss = 0.05706225
Iteration 390, loss = 0.05673734
Iteration 391, loss = 0.05641512
Iteration 392, loss = 0.05609560
Iteration 393, loss = 0.05577877
Iteration 394, loss = 0.05546461
Iteration 395, loss = 0.05515312
Iteration 396, loss = 0.05484428
Iteration 397, loss = 0.05453807
Iteration 398, loss = 0.05423449
Iteration 399, loss = 0.05393350
Iteration 400, loss = 0.05363509
Iteration 401, loss = 0.05334122
Iteration 402, loss = 0.05305020
Iteration 403, loss = 0.05276147
Iteration 404, loss = 0.05247504
Iteration 405, loss = 0.05219089
Iteration 406, loss = 0.05190902
Iteration 407, loss = 0.05162940
Iteration 408, loss = 0.05135204
Iteration 409, loss = 0.05107690
Iteration 410, loss = 0.05080398
Iteration 411, loss = 0.05053325
Iteration 412, loss = 0.05026613
Iteration 413, loss = 0.05000138
Iteration 414, loss = 0.04973857
Iteration 415, loss = 0.04947772
Iteration 416, loss = 0.04921874
Iteration 417, loss = 0.04896171
Iteration 418, loss = 0.04870661
Iteration 419, loss = 0.04845345
Iteration 420, loss = 0.04820286
Iteration 421, loss = 0.04795488
Iteration 422, loss = 0.04770841
Iteration 423, loss = 0.04746367
Iteration 424, loss = 0.04722069
Iteration 425, loss = 0.04697946
Iteration 426, loss = 0.04673998
Iteration 427, loss = 0.04650329
Iteration 428, loss = 0.04626837
Iteration 429, loss = 0.04603508
Iteration 430, loss = 0.04580341
Iteration 431, loss = 0.04557337
Iteration 432, loss = 0.04534494
Iteration 433, loss = 0.04511856
Iteration 434, loss = 0.04489434
Iteration 435, loss = 0.04467163
Iteration 436, loss = 0.04445044
Iteration 437, loss = 0.04423077
Iteration 438, loss = 0.04401290
Iteration 439, loss = 0.04379698
Iteration 440, loss = 0.04358249
Iteration 441, loss = 0.04336945
Iteration 442, loss = 0.04315825
Iteration 443, loss = 0.04294872
Iteration 444, loss = 0.04274056
Iteration 445, loss = 0.04253379
Iteration 446, loss = 0.04232910
Iteration 447, loss = 0.04212574
Iteration 448, loss = 0.04192371
Iteration 449, loss = 0.04172300
Iteration 450, loss = 0.04152362
Iteration 451, loss = 0.04132642
Iteration 452, loss = 0.04113047
Iteration 453, loss = 0.04093574
Iteration 454, loss = 0.04074225
Iteration 455, loss = 0.04055000
Iteration 456, loss = 0.04035913
Iteration 457, loss = 0.04017006
Iteration 458, loss = 0.03998217
Iteration 459, loss = 0.03979546
Iteration 460, loss = 0.03961009
Iteration 461, loss = 0.03942620
Iteration 462, loss = 0.03924342
Iteration 463, loss = 0.03906209
Iteration 464, loss = 0.03888197
Iteration 465, loss = 0.03870296
Iteration 466, loss = 0.03852522
Iteration 467, loss = 0.03834882
Iteration 468, loss = 0.03817348
Iteration 469, loss = 0.03799966
Iteration 470, loss = 0.03782690
Iteration 471, loss = 0.03765520
Iteration 472, loss = 0.03748454
Iteration 473, loss = 0.03731544
Iteration 474, loss = 0.03714742
Iteration 475, loss = 0.03698037
Iteration 476, loss = 0.03681430
Iteration 477, loss = 0.03664944
Iteration 478, loss = 0.03648591
Iteration 479, loss = 0.03632333
Iteration 480, loss = 0.03616176
Iteration 481, loss = 0.03600154
Iteration 482, loss = 0.03584241
Iteration 483, loss = 0.03568418
Iteration 484, loss = 0.03552691
Iteration 485, loss = 0.03537090
Iteration 486, loss = 0.03521582
Iteration 487, loss = 0.03506206
Iteration 488, loss = 0.03490939
Iteration 489, loss = 0.03475787
Iteration 490, loss = 0.03460725
Iteration 491, loss = 0.03445778
Iteration 492, loss = 0.03430920
Iteration 493, loss = 0.03416147
Iteration 494, loss = 0.03401502
Iteration 495, loss = 0.03386944
Iteration 496, loss = 0.03372468
Iteration 497, loss = 0.03358077
Iteration 498, loss = 0.03343782
Iteration 499, loss = 0.03329597
Iteration 500, loss = 0.03315488
Iteration 501, loss = 0.03301475
Iteration 502, loss = 0.03287557
Iteration 503, loss = 0.03273717
Iteration 504, loss = 0.03259973
Iteration 505, loss = 0.03246317
Iteration 506, loss = 0.03232734
Iteration 507, loss = 0.03219265
Iteration 508, loss = 0.03205877
Iteration 509, loss = 0.03192570
Iteration 510, loss = 0.03179339
Iteration 511, loss = 0.03166195
Iteration 512, loss = 0.03153149
Iteration 513, loss = 0.03140170
Iteration 514, loss = 0.03127278
Iteration 515, loss = 0.03114472
Iteration 516, loss = 0.03101736
Iteration 517, loss = 0.03089081
Iteration 518, loss = 0.03076508
Iteration 519, loss = 0.03064007
Iteration 520, loss = 0.03051586
Iteration 521, loss = 0.03039245
Iteration 522, loss = 0.03026974
Iteration 523, loss = 0.03014785
Iteration 524, loss = 0.03002654
Iteration 525, loss = 0.02990578
Iteration 526, loss = 0.02978580
Iteration 527, loss = 0.02966653
Iteration 528, loss = 0.02954784
Iteration 529, loss = 0.02943010
Iteration 530, loss = 0.02931297
Iteration 531, loss = 0.02919643
Iteration 532, loss = 0.02908051
Iteration 533, loss = 0.02896522
Iteration 534, loss = 0.02885076
Iteration 535, loss = 0.02873685
Iteration 536, loss = 0.02862379
Iteration 537, loss = 0.02851131
Iteration 538, loss = 0.02839942
Iteration 539, loss = 0.02828810
Iteration 540, loss = 0.02817756
Iteration 541, loss = 0.02806769
Iteration 542, loss = 0.02795833
Iteration 543, loss = 0.02784959
Iteration 544, loss = 0.02774161
Iteration 545, loss = 0.02763417
Iteration 546, loss = 0.02752735
Iteration 547, loss = 0.02742117
Iteration 548, loss = 0.02731562
Iteration 549, loss = 0.02721068
Iteration 550, loss = 0.02710626
Iteration 551, loss = 0.02700259
Iteration 552, loss = 0.02689950
Iteration 553, loss = 0.02679692
Iteration 554, loss = 0.02669510
Iteration 555, loss = 0.02659385
Iteration 556, loss = 0.02649312
Iteration 557, loss = 0.02639291
Iteration 558, loss = 0.02629321
Iteration 559, loss = 0.02619423
Iteration 560, loss = 0.02609577
Iteration 561, loss = 0.02599787
Iteration 562, loss = 0.02590057
Iteration 563, loss = 0.02580375
Iteration 564, loss = 0.02570756
Iteration 565, loss = 0.02561189
Iteration 566, loss = 0.02551670
Iteration 567, loss = 0.02542201
Iteration 568, loss = 0.02532791
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=11, learning_rate='constant',
              learning_rate_init=0.001, max_fun=15000, max_iter=1000,
              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,
              power_t=0.5, random_state=40, shuffle=True, solver='adam',
              tol=0.0001, validation_fraction=0.1, verbose=True,
              warm_start=False)
pred=clf.predict(x_Test)
print(classification_report(y_Test,pred))
              precision    recall  f1-score   support

           1       1.00      1.00      1.00        30
           2       1.00      0.97      0.99        35
           3       0.96      1.00      0.98        24

    accuracy                           0.99        89
   macro avg       0.99      0.99      0.99        89
weighted avg       0.99      0.99      0.99        89

count = 0
a=y_Test.values
for i in range(len(pred)):
    if pred[i] == a[i]:
        count = count + 1
count
88
len(pred)
89
print("Test Accuracy:",(count/len(pred)) * 100)
Test Accuracy: 98.87640449438202
print("Training set score: %f" % clf.score(x_Train, y_Train))
Training set score: 1.000000
print("Test set score: %f" % clf.score(x_Test, y_Test))
Test set score: 0.988764